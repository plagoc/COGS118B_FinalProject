{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining for Movie Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairly easy since I already downloaded about 40 screenplays, all that has to be done is to conver them from pdf into text such that it becomes easier for us to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfminer --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages that would allow us to convert them to text\n",
    "import numpy as np\n",
    "import pdfminer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "!nltk.download('stopwords')\n",
    "!nltk.download('punkt')\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac and Juptyer Notebook\n",
    "# Get the new screenplay names so that we could open them\n",
    "screenplayNameTxt = !ls MovieScreenplays/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"# For Windows\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "\n",
    "# screenplayNameTxt = [f for f in listdir('./MovieScreenplays') if isfile(join('MovieScreenplays', f))]\n",
    "# screenplayNameTxt\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the contents of the string\n",
    "screenplayTxt = []\n",
    "\n",
    "for name in screenplayNameTxt:\n",
    "    test = open(name, 'r', encoding=\"utf8\")\n",
    "    c = test.read()\n",
    "    screenplayTxt.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old data Cleaning Method that Didn't Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# word_tokens = word_tokenize(deadpool_script)\n",
    "\n",
    "# filtered_script = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "# filtered_scripts = []\n",
    "# for i in np.arange(len(screenplayTxt)):\n",
    "#     word_tokens = word_tokenize(screenplayTxt[i])\n",
    "#     filtered_scripts.append([w for w in word_tokens if not w in stop_words])\n",
    "\n",
    "# outfile = open('sample1.txt' ,'w+', encoding=\"utf8\")\n",
    "# for i in range(len(filtered_scripts[39])):\n",
    "#      outfile.write(filtered_scripts[39])\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try with Single Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove a lot of stuff that we don't want such as © symbols and numbers. Also don't want the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the script. At least for DEADPOOL for nowd\n",
    "deadpool_script = screenplayTxt[11]\n",
    "print(deadpool_script[:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with Multiple Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with multiple scripts now\n",
    "multiple_scripts = ['']\n",
    "\n",
    "for i in np.arange(len(screenplayTxt)):\n",
    "    if i == 11 or i == 22 or i == 32 or i == 1 or i == 15:\n",
    "        multiple_scripts[0] += screenplayTxt[i]\n",
    "        \n",
    "multiple_script = multiple_scripts[0]\n",
    "print(multiple_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# load document ; Here is where you change between single or multiple scripts\n",
    "doc = multiple_script\n",
    "\n",
    "# Remove symbols and reoccuring phrases that just describe the scene\n",
    "doc = doc.replace('©', '')\n",
    "doc = doc.replace('(CONTINUED)', '')\n",
    "doc = doc.replace('CONTINUED:', '')\n",
    "doc = doc.replace('Deadpool    Final Shooting Script    11/16/15      ', '')\n",
    "doc = doc.replace(' .', '')\n",
    "doc = doc.replace(' ()', '')\n",
    "doc = doc.replace('(ALT:)', '')\n",
    "doc = doc.replace('(CONT\\'D)', '')\n",
    "doc = doc.replace('(CONT’D)', '')\n",
    "doc = doc.replace('VX', '')\n",
    "doc = doc.replace('VC', '')\n",
    "doc = doc.replace('/', '')\n",
    "\n",
    "# Get rid of any digits\n",
    "remove_digits = str.maketrans(' ', ' ', digits)\n",
    "doc = doc.translate(remove_digits)\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'Multiple2.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
